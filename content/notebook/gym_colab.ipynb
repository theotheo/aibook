{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym_colab.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/theotheo/aibook/blob/master/gym_colab.ipynb)\n",
        "\n",
        "\n",
        "# Gym on Colab: https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "\n",
        "!apt-get update \n",
        "!apt-get install -y xvfb python-opengl ffmpeg cmake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade setuptools pyvirtualdisplay ez_setup \n",
        "!pip install gym[atari]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "# gymlogger.set_level(40) #error only"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M",
        "colab_type": "text"
      },
      "source": [
        "# Gym\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"MsPacman-v0\"\n",
        "env_name = 'CartPole-v0'\n",
        "# env_name = 'FrozenLake-v0'\n",
        "env = gym.make(env_name)\n",
        "print(env)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lNdVT7wMxBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW5OoE6BJ-b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "env = wrap_env(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(env.action_space, env.observation_space, env.reward_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "    observation, reward, is_done, info = env.step(action)\n",
        "    env.render()\n",
        "        \n",
        "    if is_done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVTqrt8kNkbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "class EpisodicAgent(object):\n",
        "    \"\"\"\n",
        "    Episodic agent is a simple nearest-neighbor based agent:\n",
        "    - At training time it remembers all tuples of (state, action, reward).\n",
        "    - After each episode it computes the empirical value function based \n",
        "        on the recorded rewards in the episode.\n",
        "    - At test time it looks up k-nearest neighbors in the state space \n",
        "        and takes the action that most often leads to highest average value.\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "        assert isinstance(action_space, gym.spaces.discrete.Discrete), 'unsupported action space for now.'\n",
        "\n",
        "        # options\n",
        "        self.epsilon = 1.0 # probability of choosing a random action\n",
        "        self.epsilon_decay = 0.98 # decay of epsilon per episode\n",
        "        self.epsilon_min = 0\n",
        "        self.nnfind = 500 # how many nearest neighbors to consider in the policy?\n",
        "        self.mem_needed = 500 # amount of data to have before we can start exploiting\n",
        "        self.mem_size = 50000 # maximum size of memory\n",
        "        self.gamma = 0.95 # discount factor\n",
        "\n",
        "        # internal vars\n",
        "        self.iter = 0\n",
        "        self.mem_pointer = 0 # memory pointer\n",
        "        self.max_pointer = 0\n",
        "        self.db = None # large array of states seen\n",
        "        self.dba = {} # actions taken\n",
        "        self.dbr = {} # rewards obtained at all steps\n",
        "        self.dbv = {} # value function at all steps, computed retrospectively\n",
        "        self.ep_start_pointer = 0\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        assert isinstance(observation, np.ndarray) and observation.ndim == 1, 'unsupported observation type for now.'\n",
        "\n",
        "        if self.db is None:\n",
        "            # lazy initialization of memory\n",
        "            self.db = np.zeros((self.mem_size, observation.size))\n",
        "            self.mem_pointer = 0\n",
        "            self.ep_start_pointer = 0\n",
        "\n",
        "        # we have enough data, we want to explore, and we have seen at least one episode already (so values were computed)\n",
        "        if self.iter > self.mem_needed and np.random.rand() > self.epsilon and self.dbv:\n",
        "            # exploit: find the few closest states and pick the action that led to highest rewards\n",
        "            # 1. find k nearest neighbors\n",
        "            ds = np.sum((self.db[:self.max_pointer] - observation)**2, axis=1) # L2 distance\n",
        "            ix = np.argsort(ds) # sorts ascending by distance\n",
        "            ix = ix[:min(len(ix), self.nnfind)] # crop to only some number of nearest neighbors\n",
        "            \n",
        "            # find the action that leads to most success. do a vote among actions\n",
        "            adict = {}\n",
        "            ndict = {}\n",
        "            for i in ix:\n",
        "                vv = self.dbv[i]\n",
        "                aa = self.dba[i]\n",
        "                vnew = adict.get(aa, 0) + vv\n",
        "                adict[aa] = vnew\n",
        "                ndict[aa] = ndict.get(aa, 0) + 1\n",
        "\n",
        "            for a in adict: # normalize by counts\n",
        "                adict[a] = adict[a] / ndict[a]\n",
        "\n",
        "            its = [(y,x) for x,y in adict.items()]\n",
        "            its.sort(reverse=True) # descending\n",
        "            a = its[0][1]\n",
        "\n",
        "        else:\n",
        "            # explore: do something random\n",
        "            a = self.action_space.sample()\n",
        "\n",
        "        # record move to database\n",
        "        if self.mem_pointer < self.mem_size:\n",
        "            self.db[self.mem_pointer] = observation # save the state\n",
        "            self.dba[self.mem_pointer] = a # and the action we took\n",
        "            self.dbr[self.mem_pointer-1] = reward # and the reward we obtained last time step\n",
        "            self.dbv[self.mem_pointer-1] = 0\n",
        "        self.mem_pointer += 1\n",
        "        self.iter += 1\n",
        "\n",
        "        if done: # episode Ended;\n",
        "\n",
        "            # compute the estimate of the value function based on this rollout\n",
        "            v = 0\n",
        "            for t in reversed(range(self.ep_start_pointer, self.mem_pointer)):\n",
        "                v = self.gamma * v + self.dbr.get(t,0)\n",
        "                self.dbv[t] = v\n",
        "\n",
        "            self.ep_start_pointer = self.mem_pointer\n",
        "            self.max_pointer = min(max(self.max_pointer, self.mem_pointer), self.mem_size)\n",
        "\n",
        "            # decay exploration probability\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "            self.epsilon = max(self.epsilon, self.epsilon_min) # cap at epsilon_min\n",
        "\n",
        "            print( 'memory size: ', self.mem_pointer)\n",
        "\n",
        "        return a\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPiiVphpN131",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = EpisodicAgent(env.action_space)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IDIeEnYa0Yv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "episode_count = 500\n",
        "max_steps = 200\n",
        "reward = 0\n",
        "done = False\n",
        "sum_reward_running = 0\n",
        "\n",
        "for i in range(episode_count):\n",
        "    ob = env.reset()\n",
        "    sum_reward = 0\n",
        "\n",
        "    for j in range(max_steps):\n",
        "        action = agent.act(ob, reward, done)\n",
        "        ob, reward, done, _ = env.step(action)\n",
        "        sum_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    sum_reward_running = sum_reward_running * 0.95 + sum_reward * 0.05\n",
        "    print( '%d running reward: %f' % (i, sum_reward_running))\n",
        "\n",
        "# Dump monitor info to disk\n",
        "# env.monitor.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyOXsq8RGZ09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}